{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Day 3\n",
    "\n",
    "### Expert Question Answerer for InsureLLM\n",
    "\n",
    "LangChain 1.0 implementation of a RAG pipeline.\n",
    "\n",
    "Using the VectorStore we created last time (with HuggingFace `all-MiniLM-L6-v2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# from langchain_ollama import ChatOllama\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL = \"gpt-4.1-nano\"\n",
    "DB_NAME = \"vector_db\"\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Chroma; use Hugging Face all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma(persist_directory=DB_NAME, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the 2 key LangChain objects: retriever and llm\n",
    "\n",
    "#### A sidebar on \"temperature\":\n",
    "- Controls how diverse the output is\n",
    "- A temperature of 0 means that the output should be predictable\n",
    "- Higher temperature for more variety in answers\n",
    "\n",
    "Some people describe temperature as being like 'creativity' but that's not quite right\n",
    "- It actually controls which tokens get selected during inference\n",
    "- temperature=0 means: always select the token with highest probability\n",
    "- temperature=1 usually means: a token with 10% probability should be picked 10% of the time\n",
    "\n",
    "Note: a temperature of 0 doesn't mean outputs will always be reproducible. You also need to set a random seed. We will do that in weeks 6-8. (Even then, it's not always reproducible.)\n",
    "\n",
    "Note 2: if you want creativity, use the System Prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "llm = ChatOpenAI(temperature=0, model_name=MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These LangChain objects implement the method `invoke()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='180616b2-6ac5-4623-9965-7d56d09be6e8', metadata={'Header 1': 'Avery Lancaster', 'Header 2': 'Other HR Notes', 'doc_type': 'employees', 'source': 'knowledge-base/employees/Avery Lancaster.md'}, page_content='## Other HR Notes\\n- **Professional Development**: Avery has actively participated in leadership training programs and industry conferences, representing Insurellm and fostering partnerships.\\n- **Diversity & Inclusion Initiatives**: Avery has championed a commitment to diversity in hiring practices, seeing visible improvements in team representation since 2021.\\n- **Work-Life Balance**: Feedback revealed concerns regarding work-life balance, which Avery has approached by implementing flexible working conditions and ensuring regular check-ins with the team.'),\n",
       " Document(id='5ac7bd80-507f-49b8-96eb-856eaadf0d24', metadata={'Header 2': 'Insurellm Career Progression', 'source': 'knowledge-base/employees/Avery Lancaster.md', 'doc_type': 'employees', 'Header 1': 'Avery Lancaster'}, page_content='- **2013 - 2015**: Senior Product Manager at Innovate Insurance Solutions\\nBefore launching Insurellm, Avery was a leading Senior Product Manager at Innovate Insurance Solutions, where she developed groundbreaking insurance products aimed at the tech sector.  \\n- **2010 - 2013**: Business Analyst at Edge Analytics\\nPrior to joining Innovate, Avery worked as a Business Analyst, focusing on market trends and consumer preferences in the insurance space. This position laid the groundwork for Averyâ€™s future entrepreneurial endeavors.'),\n",
       " Document(id='5f9f459a-1d33-4bcf-b1b9-0d6bb52104ee', metadata={'Header 2': 'Annual Performance History', 'doc_type': 'employees', 'Header 1': 'Avery Lancaster', 'source': 'knowledge-base/employees/Avery Lancaster.md'}, page_content=\"- **2021**: **Exceptional**\\nAvery's decisive transition to remote work and rapid adoption of digital tools led to record-high customer satisfaction levels and increased sales.  \\n- **2022**: **Satisfactory**\\nAvery focused on rebuilding team dynamics and addressing employee concerns, leading to overall improvement despite a saturated market.  \\n- **2023**: **Exceeds Expectations**\\nMarket leadership was regained with innovative approaches to personalized insurance solutions. Avery is now recognized in industry publications as a leading voice in Insurance Tech innovation.\"),\n",
       " Document(id='1668b342-bbf2-42a0-bd16-2632d24ce696', metadata={'source': 'knowledge-base/employees/Avery Lancaster.md', 'doc_type': 'employees', 'Header 1': 'Avery Lancaster', 'Header 2': 'Other HR Notes'}, page_content=\"- **Community Engagement**: Avery led community outreach efforts, focusing on financial literacy programs, particularly aimed at underserved populations, improving Insurellm's corporate social responsibility image.  \\nAvery Lancaster has demonstrated resilience and adaptability throughout her career at Insurellm, positioning the company as a key player in the insurance technology landscape.\")]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Who is Avery?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Could you please provide more context or specify which Avery you are referring to? There are many individuals and characters named Avery.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 11, 'total_tokens': 35, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7f8eb7d1f9', 'id': 'chatcmpl-CxyHHqIvFB1g9MSi86OQc3UtJstNo', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019bbd65-56f7-7c71-aa49-1b96a00ee681-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 11, 'output_tokens': 24, 'total_tokens': 35, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"Who is Avery?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to put this together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a knowledgeable, friendly assistant representing the company Insurellm.\n",
    "You are chatting with a user about Insurellm.\n",
    "If relevant, use the given context to answer any question.\n",
    "If you don't know the answer, say so.\n",
    "Context:\n",
    "{context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str, history):\n",
    "    docs = retriever.invoke(question)\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    system_prompt = SYSTEM_PROMPT_TEMPLATE.format(context=context)\n",
    "    response = llm.invoke([SystemMessage(content=system_prompt), HumanMessage(content=question)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Insurellm, we offer a range of products designed to support insurance agencies in various aspects of their business. These include:\n",
      "\n",
      "- Competitive benchmarking to help you understand your market position\n",
      "- Consumer behavior insights to better tailor your offerings\n",
      "- Product mix recommendations to optimize your portfolio\n",
      "- Reputation management tools, including review collection, response management, review showcase, and negative review mediation support\n",
      "- Marketing support with co-branded materials, email campaigns, social media suggestions, seasonal campaigns, and referral program integration\n",
      "\n",
      "If you'd like more details about any specific product or service, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# the_answer = answer_question(\"Who is Averi Lancaster?\", [])\n",
    "the_answer = answer_question(\"What products do you offer?\", [])\n",
    "\n",
    "print(the_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What could possibly come next? ðŸ˜‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frankbarnhouse/GitHub Repos/llm_engineering/.venv/lib/python3.12/site-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(answer_question).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admit it - you thought RAG would be more complicated than that!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
