{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not openai_api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not openai_api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n",
    "\n",
    "# set up OpenAI client\n",
    "gpt = OpenAI()\n",
    "\n",
    "# set up Ollama client\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10cc9c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "    You are a helpful technical assistant that will respond with a clear, concise, and easy to understand explanation to the user's technical question.\n",
    "    Respond in markdown. Do not wrap the markdown in a code block - respond just with the markdown.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "\n",
    "stream = gpt.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "response = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    response += chunk.choices[0].delta.content or ''\n",
    "    update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "    \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This code snippet is using the OpenAI GPT API to create a chat completion in a streaming manner. Here's a detailed breakdown of what each part does and why:\n",
       "\n",
       "### Code Breakdown\n",
       "\n",
       "1. **Creating a Chat Completion Stream**:\n",
       "   ```python\n",
       "   stream = gpt.chat.completions.create(\n",
       "       model=MODEL_GPT,\n",
       "       messages=[\n",
       "           {\"role\": \"system\", \"content\": system_prompt},\n",
       "           {\"role\": \"user\", \"content\": question}\n",
       "       ],\n",
       "       stream=True\n",
       "   )\n",
       "   ```\n",
       "   - `gpt.chat.completions.create(...)`: This function is called to initiate a chat completion request.\n",
       "   - `model=MODEL_GPT`: Specifies the GPT model you want to use (e.g., \"gpt-3.5-turbo\").\n",
       "   - `messages=[...]`: This is a list of messages that establish the context for the chat. The messages include:\n",
       "     - A \"system\" message that sets the behavior of the assistant using `system_prompt`.\n",
       "     - A \"user\" message containing the `question` that the user is asking.\n",
       "   - `stream=True`: This flag enables streaming, allowing the response to be received in chunks as they are generated rather than waiting for the entire response.\n",
       "\n",
       "2. **Initializing the Response and Display**:\n",
       "   ```python\n",
       "   response = \"\"\n",
       "   display_handle = display(Markdown(\"\"), display_id=True)\n",
       "   ```\n",
       "   - `response = \"\"`: Initializes an empty string to accumulate the response chunks received from the API.\n",
       "   - `display_handle = display(Markdown(\"\"), display_id=True)`: Sets up a display handle for real-time updating of the response in a Markdown format. The `display_id=True` ensures that the output can be updated in place without having to clear the entire output area.\n",
       "\n",
       "3. **Processing the Streamed Response**:\n",
       "   ```python\n",
       "   for chunk in stream:\n",
       "       response += chunk.choices[0].delta.content or ''\n",
       "       update_display(Markdown(response), display_id=display_handle.display_id)\n",
       "   ```\n",
       "   - `for chunk in stream`: This loop iterates over each chunk of data received from the streaming response.\n",
       "   - `response += chunk.choices[0].delta.content or ''`: Here, the code appends the new content from the current chunk to the `response` string. The `delta.content` contains the newly generated text by the model.\n",
       "   - `update_display(Markdown(response), display_id=display_handle.display_id)`: This updates the displayed Markdown content in real-time with the accumulated response. The use of `display_id` allows the output to be updated without creating new display outputs.\n",
       "\n",
       "### Why This Code is Useful\n",
       "- **Real-time Interaction**: Streaming allows users to see the response develop gradually, which can enhance the interactive experience.\n",
       "- **Dynamic Updates**: By updating the display in real-time, the user receives immediate feedback, making it feel more conversational and responsive.\n",
       "- **Structured Responses**: The use of structured messages (system and user roles) helps in maintaining context for the conversation, improving the quality of the responses generated by the model.\n",
       "\n",
       "In summary, this code facilitates real-time interaction with a GPT model in a chat-like format, making it suitable for applications such as chatbots or interactive Q&A systems."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "stream = gpt.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "response = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    response += chunk.choices[0].delta.content or ''\n",
    "    update_display(Markdown(response), display_id=display_handle.display_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ollama is running'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "# Check if Llama 3.2 is running\n",
    "requests.get(\"http://localhost:11434\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "573b8e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# Pull down Llama 3.2\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18717a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**GPT-3 Chat Completion Code Explanation**\n",
       "\n",
       "This code is used to initiate a chat completion session with the GPT-3 AI model through the `gpt.chat.completions` API. Here's a step-by-step breakdown of what it does:\n",
       "\n",
       "1. **Initialization**\n",
       "   - It sets up an HTTP connection to create a chat completion session. The first argument, `model=MODEL_GPT`, specifies that the conversation should use the GPT model.\n",
       "\n",
       "2. **Preparing Chat Prompts**\n",
       "   - It constructs two messages: one as \"system\" and another as \"user\". These messages are essentially predefined chat prompts.\n",
       "     - In this example, the \"system\" message is a blank line (`Markdown(\"\")`), and the \"user\" message is an unknown question (`question`). This suggests that this code may be part of a dynamic or random question generation process.\n",
       "   - The format `\"role\": \"system\", \"content\"` and `\"role\": \"user\", \"content\"` indicates the AI's role in the conversation. \n",
       "\n",
       "3. **Starting the Conversation**\n",
       "   - It sets up an HTTP connection to display data sent by the API, which is displayed below the chat window.\n",
       "\n",
       "4. **Handling Response Chunks**\n",
       "   - The loop iterates over chunks of response from the server:\n",
       "     - Each chunk includes several pieces of information such as all responses (\"all`).choices\"). In this example, it only uses the first (most likely) one.\n",
       "    - The chosen answer (`chunk.choices[0].delta.content`) is added to a string that stores the final chat response.\n",
       "\n",
       "5. **Updating Chat Window Display**\n",
       "   - Every iteration of the loop updates the display with the new responses by creating an updated Markdown instance that includes the newest addition and then displaying it using `update_display`.\n",
       "\n",
       "This code effectively initiates a GPT-3 chat conversation, sending a predefined question at startup, waiting for the AI's response, and printing that input after which we manually type it down."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=MODEL_LLAMA, messages=[{\"role\":\"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": question}])\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
